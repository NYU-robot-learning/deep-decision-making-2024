"use strict";(self.webpackChunkdeep_rl_2024=self.webpackChunkdeep_rl_2024||[]).push([[33],{9366:(t,e,s)=>{s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>n,default:()=>x,frontMatter:()=>d,metadata:()=>c,toc:()=>l});var i=s(5893),r=s(1151);const d={id:"lectures",title:"Course Schedule"},n=void 0,c={id:"lectures",title:"Course Schedule",description:"| wk | Lecture | Notes | Links | Reading material |",source:"@site/docs/lectures.md",sourceDirName:".",slug:"/lectures",permalink:"/deep-decision-making-2024/docs/lectures",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/lectures.md",tags:[],version:"current",frontMatter:{id:"lectures",title:"Course Schedule"},sidebar:"tutorialSidebar",previous:{title:"Assignments",permalink:"/deep-decision-making-2024/docs/assignments"},next:{title:"Class Logistics",permalink:"/deep-decision-making-2024/docs/logistics"}},a={},l=[];function o(t){const e={table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.a)(),...t.components};return(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"wk"}),(0,i.jsx)(e.th,{children:"Lecture"}),(0,i.jsx)(e.th,{children:"Notes"}),(0,i.jsx)(e.th,{children:"Links"}),(0,i.jsx)(e.th,{children:"Reading material"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"1/26"}),(0,i.jsx)(e.td,{children:"Introduction to Deep Decision Making"}),(0,i.jsxs)(e.td,{children:["Part 1: What is this class about? ",(0,i.jsx)("br",{})," Part 2: Why is decision making hard?"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"2/2"}),(0,i.jsx)(e.td,{children:"Supervised Learning for Decision Making"}),(0,i.jsxs)(e.td,{children:["Part 1: Training Neural Networks ",(0,i.jsx)("br",{})," Part 2: Variants of Behavior Cloning policies"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"2/9"}),(0,i.jsx)(e.td,{children:"[Tutorial] Supervised Learning for Decision Making"}),(0,i.jsx)(e.td,{children:"Setting up decision making environments and model training"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"2/16"}),(0,i.jsx)(e.td,{children:"Case studies of supervised decision making"}),(0,i.jsx)(e.td,{children:"Examples of supervised learning working in the real world"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"2/23"}),(0,i.jsx)(e.td,{children:"Decision making without expert data"}),(0,i.jsxs)(e.td,{children:["Part 1: Formalism for Bandit problem ",(0,i.jsx)("br",{})," Part 2: Algorithms for Bandit problems"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3/1"}),(0,i.jsx)(e.td,{children:"Sequential Decision making"}),(0,i.jsxs)(e.td,{children:["Part 1: Motivation and formalism ",(0,i.jsx)("br",{})," Part 2: Core concepts of value and policy iteration"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3/8"}),(0,i.jsx)(e.td,{children:"Q-learning: from tables to Atari"}),(0,i.jsxs)(e.td,{children:["Part 1: Why Q function? ",(0,i.jsx)("br",{})," Part 2: Deep Q functions: What goes wrong and how to make them work? ",(0,i.jsx)("br",{})," Part 3: Variants of DQN"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3/15"}),(0,i.jsx)(e.td,{children:"Policy Optimization"}),(0,i.jsxs)(e.td,{children:["Part 1: MC-based optimization (CEM) ",(0,i.jsx)("br",{})," Part 2: Differentiable versions (REINFORCE) ",(0,i.jsx)("br",{})," Part 3: Trust region / proximal policy optimization"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3/22"}),(0,i.jsx)(e.td,{children:"SPRING BREAK"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3/29"}),(0,i.jsx)(e.td,{children:"[Tutorial] Setting up RL environments and training simple RL policies."}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"4/5"}),(0,i.jsx)(e.td,{children:"Decision making with world models"}),(0,i.jsxs)(e.td,{children:["Part 1: Classical approaches (LQR / iLQR / DDP) ",(0,i.jsx)("br",{})," Part 2: Model-based RL ",(0,i.jsx)("br",{})," Part 3: case study: Dreamer v3"]}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"4/12"}),(0,i.jsx)(e.td,{children:"Decision making with Tree Search"}),(0,i.jsx)(e.td,{children:"MCTC (AlphaGo, AlphaZero)"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"4/19"}),(0,i.jsx)(e.td,{children:"Revisiting Decision making with expert data"}),(0,i.jsx)(e.td,{children:"Inverse RL and offline RL"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"4/26"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"5/3"}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{}),(0,i.jsx)(e.td,{})]})]})]})}function x(t={}){const{wrapper:e}={...(0,r.a)(),...t.components};return e?(0,i.jsx)(e,{...t,children:(0,i.jsx)(o,{...t})}):o(t)}},1151:(t,e,s)=>{s.d(e,{Z:()=>c,a:()=>n});var i=s(7294);const r={},d=i.createContext(r);function n(t){const e=i.useContext(d);return i.useMemo((function(){return"function"==typeof t?t(e):{...e,...t}}),[e,t])}function c(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(r):t.components||r:n(t.components),i.createElement(d.Provider,{value:e},t.children)}}}]);