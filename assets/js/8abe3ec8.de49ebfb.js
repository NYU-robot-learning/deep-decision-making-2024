"use strict";(self.webpackChunkdeep_rl_2024=self.webpackChunkdeep_rl_2024||[]).push([[33],{9366:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>n,metadata:()=>a,toc:()=>c});var r=s(5893),i=s(1151);const n={id:"lectures",title:"Course Schedule"},d=void 0,a={id:"lectures",title:"Course Schedule",description:"| wk | Lecture | Notes | Links | Reading material |",source:"@site/docs/lectures.md",sourceDirName:".",slug:"/lectures",permalink:"/deep-decision-making-2024/docs/lectures",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/lectures.md",tags:[],version:"current",frontMatter:{id:"lectures",title:"Course Schedule"},sidebar:"tutorialSidebar",previous:{title:"Assignments",permalink:"/deep-decision-making-2024/docs/assignments"},next:{title:"Class Logistics",permalink:"/deep-decision-making-2024/docs/logistics"}},o={},c=[];function l(e){const t={a:"a",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.a)(),...e.components};return(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"wk"}),(0,r.jsx)(t.th,{children:"Lecture"}),(0,r.jsx)(t.th,{children:"Notes"}),(0,r.jsx)(t.th,{children:"Links"}),(0,r.jsx)(t.th,{children:"Reading material"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"1/26"}),(0,r.jsx)(t.td,{children:"Introduction to Deep Decision Making"}),(0,r.jsxs)(t.td,{children:["Part 1: What is this class about? ",(0,r.jsx)("br",{})," Part 2: Why is decision making hard?"]}),(0,r.jsx)(t.td,{}),(0,r.jsxs)(t.td,{children:[(0,r.jsx)("br",{}),"1. ",(0,r.jsx)(t.a,{href:"http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf",children:"A framework for behavioural cloning, Bain and Sommut, 1999."}),(0,r.jsx)("br",{}),"2. ",(0,r.jsx)(t.a,{href:"http://proceedings.mlr.press/v15/ross11a/ross11a.pdf",children:"A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011."})]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"2/2"}),(0,r.jsx)(t.td,{children:"Supervised Learning for Decision Making"}),(0,r.jsxs)(t.td,{children:["Part 1: Training Neural Networks ",(0,r.jsx)("br",{})," Part 2: Variants of Behavior Cloning policies"]}),(0,r.jsx)(t.td,{}),(0,r.jsxs)(t.td,{children:[(0,r.jsx)("br",{}),"1. ",(0,r.jsx)(t.a,{href:"https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf",children:"Behavior Cloning (ALVINN)"}),(0,r.jsx)("br",{}),"2. ",(0,r.jsx)(t.a,{href:"https://lilianweng.github.io/posts/2018-08-12-vae/",children:"Variational Autoencoder"}),(0,r.jsx)("br",{}),"3. ",(0,r.jsx)(t.a,{href:"https://arxiv.org/abs/1406.2661",children:"Generative Adversarial Networks"}),(0,r.jsx)("br",{}),"4. Case study papers: ",(0,r.jsx)(t.a,{href:"https://jyopari.github.io/VINN/",children:"VINN"}),", ",(0,r.jsx)(t.a,{href:"https://blog.research.google/2022/12/rt-1-robotics-transformer-for-real.html",children:"RT-1"}),", ",(0,r.jsx)(t.a,{href:"https://dobb-e.com/",children:"Dobb-E"}),", ",(0,r.jsx)(t.a,{href:"https://implicitbc.github.io/",children:"Implicit BC"}),", ",(0,r.jsx)(t.a,{href:"https://mahis.life/bet/",children:"BeT"}),", ",(0,r.jsx)(t.a,{href:"https://play-to-policy.github.io/",children:"C-BeT"}),", ",(0,r.jsx)(t.a,{href:"https://diffusion-policy.cs.columbia.edu/",children:"Diffusion Policy"})]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"2/9"}),(0,r.jsx)(t.td,{children:"[Tutorial] Supervised Learning for Decision Making"}),(0,r.jsx)(t.td,{children:"Setting up decision making environments and model training"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"2/16"}),(0,r.jsx)(t.td,{children:"Case studies of supervised decision making"}),(0,r.jsx)(t.td,{children:"Examples of supervised learning working in the real world"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"2/23"}),(0,r.jsx)(t.td,{children:"Decision making without expert data"}),(0,r.jsxs)(t.td,{children:["Part 1: Formalism for Bandit problem ",(0,r.jsx)("br",{})," Part 2: Algorithms for Bandit problems"]}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"3/1"}),(0,r.jsx)(t.td,{children:"Sequential Decision making"}),(0,r.jsxs)(t.td,{children:["Part 1: Motivation and formalism ",(0,r.jsx)("br",{})," Part 2: Core concepts of value and policy iteration"]}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"3/8"}),(0,r.jsx)(t.td,{children:"Q-learning: from tables to Atari"}),(0,r.jsxs)(t.td,{children:["Part 1: Why Q function? ",(0,r.jsx)("br",{})," Part 2: Deep Q functions: What goes wrong and how to make them work? ",(0,r.jsx)("br",{})," Part 3: Variants of DQN"]}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"3/15"}),(0,r.jsx)(t.td,{children:"Policy Optimization"}),(0,r.jsxs)(t.td,{children:["Part 1: MC-based optimization (CEM) ",(0,r.jsx)("br",{})," Part 2: Differentiable versions (REINFORCE) ",(0,r.jsx)("br",{})," Part 3: Trust region / proximal policy optimization"]}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"3/22"}),(0,r.jsx)(t.td,{children:"SPRING BREAK"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"3/29"}),(0,r.jsx)(t.td,{children:"[Tutorial] Setting up RL environments and training simple RL policies."}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"4/5"}),(0,r.jsx)(t.td,{children:"Decision making with world models"}),(0,r.jsxs)(t.td,{children:["Part 1: Classical approaches (LQR / iLQR / DDP) ",(0,r.jsx)("br",{})," Part 2: Model-based RL ",(0,r.jsx)("br",{})," Part 3: case study: Dreamer v3"]}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"4/12"}),(0,r.jsx)(t.td,{children:"Decision making with Tree Search"}),(0,r.jsx)(t.td,{children:"MCTC (AlphaGo, AlphaZero)"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"4/19"}),(0,r.jsx)(t.td,{children:"Revisiting Decision making with expert data"}),(0,r.jsx)(t.td,{children:"Inverse RL and offline RL"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"4/26"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"5/3"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]})]})]})}function h(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},1151:(e,t,s)=>{s.d(t,{Z:()=>a,a:()=>d});var r=s(7294);const i={},n=r.createContext(i);function d(e){const t=r.useContext(n);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),r.createElement(n.Provider,{value:t},e.children)}}}]);